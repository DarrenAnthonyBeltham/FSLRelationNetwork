{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/darrenanthonybeltham/cari-rate?scriptVersionId=245944973\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"864524e0","metadata":{"execution":{"iopub.execute_input":"2025-06-17T14:03:14.850459Z","iopub.status.busy":"2025-06-17T14:03:14.850148Z","iopub.status.idle":"2025-06-17T14:13:27.887488Z","shell.execute_reply":"2025-06-17T14:13:27.88675Z"},"papermill":{"duration":613.168075,"end_time":"2025-06-17T14:13:28.012611","exception":false,"start_time":"2025-06-17T14:03:14.844536","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating dummy metadata file for demonstration.\n","Using device: cuda\n","Starting experiment with RELATION...\n","\n","Preparing domain-specific datasets...\n","\n","Overall class distribution: {'nv': 100, 'mel': 100}\n","\n","Processing domain: age_over_30_body\n","\n","Processing domain: age_under_30_body\n","Warning: Skipping domain age_under_30_body due to insufficient samples or classes.\n","\n","Processing domain: age_over_30_head_neck\n","\n","Processing domain: age_over_30_hands_feet\n","Warning: Skipping domain age_over_30_hands_feet due to insufficient samples or classes.\n","\n","Processing domain: age_over_30_limbs\n"]},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n","100%|██████████| 82.7M/82.7M [00:00<00:00, 204MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["RelationNet initialized with EfficientNetV2-S backbone.\n","\n","==================================================\n","Training model for domain: age_over_30_body\n","==================================================\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 50/50 [00:19<00:00,  2.55it/s, loss=0.699]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Train Loss: 0.6992, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n","Saved new best model with validation loss: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 50/50 [00:17<00:00,  2.85it/s, loss=0.697]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Train Loss: 0.6969, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 50/50 [00:17<00:00,  2.85it/s, loss=0.696]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Train Loss: 0.6961, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s, loss=0.696]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Train Loss: 0.6963, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 50/50 [00:17<00:00,  2.82it/s, loss=0.695]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Train Loss: 0.6947, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6: 100%|██████████| 50/50 [00:17<00:00,  2.85it/s, loss=0.693]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6, Train Loss: 0.6934, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s, loss=0.697]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7, Train Loss: 0.6967, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s, loss=0.694]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8, Train Loss: 0.6937, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s, loss=0.692]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9, Train Loss: 0.6924, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10: 100%|██████████| 50/50 [00:17<00:00,  2.84it/s, loss=0.693]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10, Train Loss: 0.6930, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n","\n","Final Test Results for domain age_over_30_body: Acc: 1.0000, AUROC: 0.0000\n","RelationNet initialized with EfficientNetV2-S backbone.\n","\n","==================================================\n","Training model for domain: age_over_30_head_neck\n","==================================================\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 50/50 [00:21<00:00,  2.36it/s, loss=0.697]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Train Loss: 0.6968, Val Loss: 0.6931, Val Acc: 0.5000, Val AUROC: 0.5000\n","Saved new best model with validation loss: 0.6931\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 50/50 [00:21<00:00,  2.37it/s, loss=0.7]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Train Loss: 0.7001, Val Loss: 0.6931, Val Acc: 0.5000, Val AUROC: 0.5000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 50/50 [00:21<00:00,  2.35it/s, loss=0.695]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Train Loss: 0.6950, Val Loss: 0.6931, Val Acc: 0.5000, Val AUROC: 0.5000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 50/50 [00:21<00:00,  2.34it/s, loss=0.696]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Train Loss: 0.6961, Val Loss: 0.6931, Val Acc: 0.5000, Val AUROC: 0.5000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 50/50 [00:21<00:00,  2.36it/s, loss=0.693]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Train Loss: 0.6926, Val Loss: 0.6931, Val Acc: 0.5000, Val AUROC: 0.5000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6: 100%|██████████| 50/50 [00:21<00:00,  2.36it/s, loss=0.695]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6, Train Loss: 0.6952, Val Loss: 0.6931, Val Acc: 0.5000, Val AUROC: 0.5000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7: 100%|██████████| 50/50 [00:21<00:00,  2.34it/s, loss=0.694]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7, Train Loss: 0.6941, Val Loss: 0.6931, Val Acc: 0.5000, Val AUROC: 0.5000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8: 100%|██████████| 50/50 [00:21<00:00,  2.35it/s, loss=0.693]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8, Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5000, Val AUROC: 0.5000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9: 100%|██████████| 50/50 [00:21<00:00,  2.35it/s, loss=0.694]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9, Train Loss: 0.6938, Val Loss: 0.6931, Val Acc: 0.5000, Val AUROC: 0.5000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10: 100%|██████████| 50/50 [00:21<00:00,  2.35it/s, loss=0.693]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10, Train Loss: 0.6932, Val Loss: 0.6931, Val Acc: 0.5000, Val AUROC: 0.5000\n","\n","Final Test Results for domain age_over_30_head_neck: Acc: 0.5000, AUROC: 0.5000\n","RelationNet initialized with EfficientNetV2-S backbone.\n","\n","==================================================\n","Training model for domain: age_over_30_limbs\n","==================================================\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s, loss=0.743]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Train Loss: 0.7427, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n","Saved new best model with validation loss: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 50/50 [00:13<00:00,  3.84it/s, loss=0.709]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Train Loss: 0.7092, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 50/50 [00:12<00:00,  3.88it/s, loss=0.711]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Train Loss: 0.7106, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 50/50 [00:12<00:00,  3.88it/s, loss=0.703]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Train Loss: 0.7025, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 50/50 [00:13<00:00,  3.83it/s, loss=0.699]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Train Loss: 0.6988, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6: 100%|██████████| 50/50 [00:12<00:00,  3.86it/s, loss=0.702]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6, Train Loss: 0.7017, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7: 100%|██████████| 50/50 [00:13<00:00,  3.84it/s, loss=0.705]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7, Train Loss: 0.7046, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8: 100%|██████████| 50/50 [00:12<00:00,  3.89it/s, loss=0.701]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8, Train Loss: 0.7006, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9: 100%|██████████| 50/50 [00:13<00:00,  3.82it/s, loss=0.692]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9, Train Loss: 0.6923, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10: 100%|██████████| 50/50 [00:12<00:00,  3.88it/s, loss=0.697]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10, Train Loss: 0.6966, Val Loss: 0.0000, Val Acc: 0.0000, Val AUROC: 0.0000\n","\n","Final Test Results for domain age_over_30_limbs: Acc: 0.0000, AUROC: 0.0000\n","RelationNet initialized with EfficientNetV2-S backbone.\n","\n","==================================================\n","Cross-domain: age_over_30_body -> age_over_30_head_neck\n","==================================================\n","Cross-domain results (age_over_30_body -> age_over_30_head_neck): Acc: 0.5000, AUROC: 0.5000\n","RelationNet initialized with EfficientNetV2-S backbone.\n","\n","==================================================\n","Cross-domain: age_over_30_body -> age_over_30_limbs\n","==================================================\n","Cross-domain results (age_over_30_body -> age_over_30_limbs): Acc: 0.0000, AUROC: 0.0000\n","RelationNet initialized with EfficientNetV2-S backbone.\n","\n","==================================================\n","Cross-domain: age_over_30_head_neck -> age_over_30_body\n","==================================================\n","Cross-domain results (age_over_30_head_neck -> age_over_30_body): Acc: 1.0000, AUROC: 0.0000\n","RelationNet initialized with EfficientNetV2-S backbone.\n","\n","==================================================\n","Cross-domain: age_over_30_head_neck -> age_over_30_limbs\n","==================================================\n","Cross-domain results (age_over_30_head_neck -> age_over_30_limbs): Acc: 0.0000, AUROC: 0.0000\n","RelationNet initialized with EfficientNetV2-S backbone.\n","\n","==================================================\n","Cross-domain: age_over_30_limbs -> age_over_30_body\n","==================================================\n","Cross-domain results (age_over_30_limbs -> age_over_30_body): Acc: 1.0000, AUROC: 0.0000\n","RelationNet initialized with EfficientNetV2-S backbone.\n","\n","==================================================\n","Cross-domain: age_over_30_limbs -> age_over_30_head_neck\n","==================================================\n","Cross-domain results (age_over_30_limbs -> age_over_30_head_neck): Acc: 0.5000, AUROC: 0.5000\n","\n","==================================================\n","EXPERIMENT SUMMARY (RELATION)\n","==================================================\n","\n","Within-domain performance:\n","  age_over_30_body: Accuracy: 1.0000, AUROC: 0.0000\n","  age_over_30_head_neck: Accuracy: 0.5000, AUROC: 0.5000\n","  age_over_30_limbs: Accuracy: 0.0000, AUROC: 0.0000\n","\n","Cross-domain performance:\n","  age_over_30_body -> age_over_30_head_neck: Accuracy: 0.5000, AUROC: 0.5000\n","  age_over_30_body -> age_over_30_limbs: Accuracy: 0.0000, AUROC: 0.0000\n","  age_over_30_head_neck -> age_over_30_body: Accuracy: 1.0000, AUROC: 0.0000\n","  age_over_30_head_neck -> age_over_30_limbs: Accuracy: 0.0000, AUROC: 0.0000\n","  age_over_30_limbs -> age_over_30_body: Accuracy: 1.0000, AUROC: 0.0000\n","  age_over_30_limbs -> age_over_30_head_neck: Accuracy: 0.5000, AUROC: 0.5000\n","\n","Experiment completed successfully!\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n","from sklearn.metrics import precision_recall_curve, auc, f1_score, precision_score, recall_score\n","from sklearn.manifold import TSNE\n","from PIL import Image\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import random\n","from collections import defaultdict\n","import seaborn as sns\n","from datetime import datetime\n","\n","# For displaying plots in the output\n","# %matplotlib inline # Uncomment if in a Jupyter environment\n","plt.rcParams['figure.figsize'] = (12, 8)\n","plt.style.use('ggplot')\n","\n","# Set random seeds for reproducibility\n","def set_seed(seed=20):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed()\n","\n","# Define paths (adjust according to your environment)\n","# Example for a local setup, change if using Kaggle/Colab\n","BASE_PATH = './' # Assuming data is in the current directory\n","DATA_PATH = os.path.join(BASE_PATH, 'HAM10000_metadata.csv')\n","IMAGE_DIR_1 = os.path.join(BASE_PATH, 'HAM10000_images_part_1/')\n","IMAGE_DIR_2 = os.path.join(BASE_PATH, 'HAM10000_images_part_2/')\n","\n","# Create dummy directories and files for demonstration if they don't exist\n","if not os.path.exists(DATA_PATH):\n","    print(\"Creating dummy metadata file for demonstration.\")\n","    dummy_data = {\n","        'lesion_id': [f'HAM_{i:07d}' for i in range(200)],\n","        'image_id': [f'ISIC_{i:07d}' for i in range(200)],\n","        'dx': ['nv'] * 100 + ['mel'] * 100,\n","        'dx_type': ['histo'] * 200,\n","        'age': list(np.random.randint(20, 80, 200)),\n","        'sex': ['male', 'female'] * 100,\n","        'localization': ['back', 'face', 'chest', 'foot', 'ear', 'lower extremity', 'upper extremity', 'abdomen', 'neck', 'scalp'] * 20\n","    }\n","    pd.DataFrame(dummy_data).to_csv(DATA_PATH, index=False)\n","if not os.path.exists(IMAGE_DIR_1): os.makedirs(IMAGE_DIR_1)\n","if not os.path.exists(IMAGE_DIR_2): os.makedirs(IMAGE_DIR_2)\n","\n","\n","# Configuration class - with domain definitions\n","class Config:\n","    img_size = 224\n","    batch_size = 32\n","    lr = 0.001\n","    n_epochs = 10  # Reduced for faster demonstration\n","    n_query = 15  # Number of query images per class\n","    n_way = 2  # Binary classification (melanoma vs nevus)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    shots = [5]  # Fixed at 5-shot for simplicity\n","    train_episodes = 50  # Reduced for faster demonstration\n","    val_episodes = 50  # Validation episodes\n","    test_episodes = 50  # Test episodes\n","    \n","    # Domain definitions to split the dataset\n","    domains = {\n","        'age_over_30_body': {'age_condition': lambda x: x > 30, \n","                             'loc_condition': lambda x: x in ['abdomen', 'back', 'chest', 'genital', 'trunk']},\n","        'age_under_30_body': {'age_condition': lambda x: x <= 30, \n","                              'loc_condition': lambda x: x in ['abdomen', 'back', 'chest', 'genital', 'trunk']},\n","        'age_over_30_head_neck': {'age_condition': lambda x: x > 30, \n","                                  'loc_condition': lambda x: x in ['ear', 'face', 'neck', 'scalp']},\n","        'age_over_30_hands_feet': {'age_condition': lambda x: x > 30, \n","                                   'loc_condition': lambda x: x in ['acral', 'foot', 'hand']},\n","        'age_over_30_limbs': {'age_condition': lambda x: x > 30, \n","                              'loc_condition': lambda x: x in ['lower extremity', 'upper extremity']}\n","    }\n","\n","config = Config()\n","print(f\"Using device: {config.device}\")\n","\n","class DynamicTaskOversampler:\n","    \"\"\"\n","    Handles dynamic oversampling at the task/episode level for few-shot learning\n","    \"\"\"\n","    def __init__(self, augmentation_strength='moderate'):\n","        self.augmentation_strength = augmentation_strength\n","        self.setup_augmentations()\n","    \n","    def setup_augmentations(self):\n","        \"\"\"Setup different levels of augmentation for synthetic samples\"\"\"\n","        if self.augmentation_strength == 'light':\n","            self.augment_transform = transforms.Compose([\n","                transforms.RandomHorizontalFlip(p=0.5),\n","                transforms.RandomRotation(15),\n","                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)\n","            ])\n","        elif self.augmentation_strength == 'moderate':\n","            self.augment_transform = transforms.Compose([\n","                transforms.RandomHorizontalFlip(p=0.5),\n","                transforms.RandomVerticalFlip(p=0.3),\n","                transforms.RandomRotation(30),\n","                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n","                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1))\n","            ])\n","        elif self.augmentation_strength == 'strong':\n","            self.augment_transform = transforms.Compose([\n","                transforms.RandomHorizontalFlip(p=0.7),\n","                transforms.RandomVerticalFlip(p=0.5),\n","                transforms.RandomRotation(45),\n","                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n","                transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.8, 1.2)),\n","                transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))\n","            ])\n","    \n","    def augment_image(self, image_tensor):\n","        \"\"\"Apply augmentation to a single image tensor\"\"\"\n","        to_pil = transforms.ToPILImage()\n","        to_tensor = transforms.ToTensor()\n","        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        \n","        # Denormalize\n","        denorm_image = image_tensor.clone()\n","        for t, m, s in zip(denorm_image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]):\n","            t.mul_(s).add_(m)\n","        \n","        pil_image = to_pil(denorm_image.clamp(0, 1))\n","        augmented_pil = self.augment_transform(pil_image)\n","        \n","        augmented_tensor = to_tensor(augmented_pil)\n","        normalized_tensor = normalize(augmented_tensor)\n","        \n","        return normalized_tensor\n","    \n","    def balance_support_set(self, support_images, support_labels, target_shots_per_class=None):\n","        unique_classes, counts = torch.unique(support_labels, return_counts=True)\n","        class_counts = dict(zip(unique_classes.tolist(), counts.tolist()))\n","        \n","        if target_shots_per_class is None:\n","            target_count = max(class_counts.values()) if class_counts else 0\n","        else:\n","            target_count = target_shots_per_class\n","            \n","        if not class_counts or target_count == 0:\n","             return support_images, support_labels\n","\n","        balanced_images = list(torch.unbind(support_images, dim=0))\n","        balanced_labels = list(torch.unbind(support_labels, dim=0))\n","        \n","        for cls_idx in class_counts.keys():\n","            current_count = class_counts[cls_idx]\n","            samples_needed = target_count - current_count\n","            \n","            if samples_needed > 0:\n","                class_indices = torch.where(support_labels == cls_idx)[0]\n","                if len(class_indices) == 0: continue\n","\n","                for _ in range(samples_needed):\n","                    source_idx = random.choice(class_indices.tolist())\n","                    source_image = support_images[source_idx]\n","                    synthetic_image = self.augment_image(source_image)\n","                    balanced_images.append(synthetic_image)\n","                    balanced_labels.append(torch.tensor(cls_idx))\n","        \n","        balanced_images_tensor = torch.stack(balanced_images)\n","        balanced_labels_tensor = torch.stack(balanced_labels)\n","        \n","        perm = torch.randperm(len(balanced_images_tensor))\n","        return balanced_images_tensor[perm], balanced_labels_tensor[perm]\n","\n","\n","class HAM10000Dataset(Dataset):\n","    def __init__(self, df, img_dir_1, img_dir_2, transform=None):\n","        self.df = df\n","        self.img_dir_1 = img_dir_1\n","        self.img_dir_2 = img_dir_2\n","        self.transform = transform\n","        \n","        self.class_map = {'nv': 0, 'mel': 1}\n","        self.df = self.df[self.df['dx'].isin(self.class_map.keys())].reset_index(drop=True)\n","        self.labels = self.df['dx'].map(self.class_map).tolist()\n","    \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        img_id = self.df.iloc[idx]['image_id']\n","        img_path = os.path.join(self.img_dir_1, f\"{img_id}.jpg\")\n","        if not os.path.exists(img_path):\n","            img_path = os.path.join(self.img_dir_2, f\"{img_id}.jpg\")\n","        \n","        try:\n","            img = Image.open(img_path).convert('RGB')\n","        except FileNotFoundError:\n","            # Create a dummy image if not found (for demonstration)\n","            img = Image.new('RGB', (config.img_size, config.img_size), color='gray')\n","            # Create a dummy file to avoid repeated errors\n","            img.save(img_path)\n","            \n","        if self.transform:\n","            img = self.transform(img)\n","            \n","        label = self.labels[idx]\n","        return img, label\n","\n","def prepare_fsl_data(config):\n","    os.makedirs(\"results\", exist_ok=True)\n","    os.makedirs(\"results/cross_domain\", exist_ok=True)\n","    \n","    print(\"\\nPreparing domain-specific datasets...\")\n","    domain_datasets = prepare_domain_datasets()\n","    \n","    fsl_data = {}\n","    for domain_name, domain_data in domain_datasets.items():\n","        fsl_data[domain_name] = {\n","            'train': {'dataset': domain_data['train_dataset'], 'indices_by_class': domain_data['train_indices_by_class'], 'df': domain_data['train_df']},\n","            'val': {'dataset': domain_data['val_dataset'], 'indices_by_class': domain_data['val_indices_by_class'], 'df': domain_data['val_df']},\n","            'test': {'dataset': domain_data['test_dataset'], 'indices_by_class': domain_data['test_indices_by_class'], 'df': domain_data['test_df']}\n","        }\n","    return fsl_data\n","\n","def preprocess_age(age):\n","    if pd.isna(age) or age <= 0:\n","        return 50  # Use median age as a fallback\n","    return age\n","\n","def get_transforms():\n","    train_transform = transforms.Compose([\n","        transforms.Resize((config.img_size, config.img_size)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.RandomRotation(10),\n","        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","    test_transform = transforms.Compose([\n","        transforms.Resize((config.img_size, config.img_size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","    return train_transform, test_transform\n","\n","def prepare_domain_datasets():\n","    df = pd.read_csv(DATA_PATH)\n","    df['age'] = df['age'].apply(preprocess_age)\n","    df = df[df['dx'].isin(['nv', 'mel'])].reset_index(drop=True)\n","    \n","    print(\"\\nOverall class distribution:\", df['dx'].value_counts().to_dict())\n","    \n","    train_transform, test_transform = get_transforms()\n","    domain_datasets = {}\n","    \n","    for domain_name, conditions in config.domains.items():\n","        print(f\"\\nProcessing domain: {domain_name}\")\n","        domain_df = df[df['age'].apply(conditions['age_condition']) & df['localization'].apply(conditions['loc_condition'])]\n","        \n","        if len(domain_df) < 20 or domain_df['dx'].nunique() < 2:\n","            print(f\"Warning: Skipping domain {domain_name} due to insufficient samples or classes.\")\n","            continue\n","            \n","        train_df, val_test_df = train_test_split(domain_df, test_size=0.4, random_state=20, stratify=domain_df['dx'])\n","        val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=20, stratify=val_test_df['dx'])\n","        \n","        train_dataset = HAM10000Dataset(train_df, IMAGE_DIR_1, IMAGE_DIR_2, transform=train_transform)\n","        val_dataset = HAM10000Dataset(val_df, IMAGE_DIR_1, IMAGE_DIR_2, transform=test_transform)\n","        test_dataset = HAM10000Dataset(test_df, IMAGE_DIR_1, IMAGE_DIR_2, transform=test_transform)\n","        \n","        domain_datasets[domain_name] = {\n","            'train_dataset': train_dataset, 'val_dataset': val_dataset, 'test_dataset': test_dataset,\n","            'train_indices_by_class': {lbl: np.where(np.array(train_dataset.labels) == lbl)[0] for lbl in [0, 1]},\n","            'val_indices_by_class': {lbl: np.where(np.array(val_dataset.labels) == lbl)[0] for lbl in [0, 1]},\n","            'test_indices_by_class': {lbl: np.where(np.array(test_dataset.labels) == lbl)[0] for lbl in [0, 1]},\n","            'train_df': train_df, 'val_df': val_df, 'test_df': test_df\n","        }\n","        # Disabling visualization for brevity in this script\n","        # visualize_dataset_split(domain_name, train_df, val_df, test_df)\n","    return domain_datasets\n","\n","# --- FSL MODEL DEFINITIONS ---\n","\n","class ProtoNet(nn.Module):\n","    def __init__(self, config_obj):\n","        super(ProtoNet, self).__init__()\n","        self.backbone = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n","        # Freeze early layers\n","        for name, param in self.backbone.named_parameters():\n","            if 'features.0.' in name or 'features.1.' in name:\n","                param.requires_grad = False\n","        \n","        self.feature_dim = self.backbone.classifier[1].in_features\n","        self.backbone.classifier = nn.Identity()\n","        \n","        self.embedding = nn.Sequential(\n","            nn.Linear(self.feature_dim, 512), nn.ReLU(), nn.Dropout(0.3),\n","            nn.Linear(512, 256), nn.Dropout(0.2)\n","        )\n","        print(f\"ProtoNet initialized with EfficientNetV2-S backbone.\")\n","\n","    def forward(self, x):\n","        features = self.backbone(x)\n","        return self.embedding(features)\n","        \n","    def calculate_prototypes(self, support_features, support_labels, n_way):\n","        prototypes = torch.zeros(n_way, support_features.shape[1]).to(support_features.device)\n","        for i in range(n_way):\n","            mask = support_labels == i\n","            if mask.sum() > 0:\n","                prototypes[i] = support_features[mask].mean(0)\n","        return prototypes\n","        \n","    def calculate_distances(self, query_features, prototypes):\n","        return torch.cdist(query_features, prototypes) ** 2\n","\n","# NEW: RelationNet Implementation\n","class RelationNet(nn.Module):\n","    def __init__(self, config_obj): # Accepts the standard config object\n","        super(RelationNet, self).__init__()\n","        self.backbone = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n","        # Freeze early layers\n","        for name, param in self.backbone.named_parameters():\n","            if name.startswith(\"features.0.\") or name.startswith(\"features.1.\") or name.startswith(\"features.2.\"):\n","                param.requires_grad = False\n","        \n","        # Get original feature dimension before modifying the classifier\n","        bb_feat_dim = self.backbone.classifier[1].in_features\n","        self.backbone.classifier = nn.Identity()\n","\n","        self.embedding_output_dim = 256\n","        self.embedding_layers = nn.Sequential(\n","            nn.Linear(bb_feat_dim, 512), nn.ReLU(), nn.Dropout(0.4),\n","            nn.Linear(512, self.embedding_output_dim), nn.Dropout(0.3)\n","        )\n","        # Relation module to compare concatenated (prototype, query) pairs\n","        self.relation_module = nn.Sequential(\n","            nn.Linear(2 * self.embedding_output_dim, 256), nn.ReLU(), nn.Dropout(0.3),\n","            nn.Linear(256, 1)\n","        )\n","        print(f\"RelationNet initialized with EfficientNetV2-S backbone.\")\n","        \n","    def get_embeddings(self, x):\n","        \"\"\"Helper function to get embeddings for t-SNE visualization.\"\"\"\n","        with torch.no_grad():\n","            features = self.backbone(x)\n","            embeddings = self.embedding_layers(features)\n","            return embeddings\n","        \n","    def forward(self, sup_img, sup_lab, qry_img, n_w, n_s):\n","        sup_feat = self.backbone(sup_img)\n","        qry_feat = self.backbone(qry_img)\n","        \n","        sup_emb = self.embedding_layers(sup_feat)\n","        qry_emb = self.embedding_layers(qry_feat)\n","        \n","        # Calculate class prototypes from support embeddings\n","        protos = []\n","        for i in range(n_w):\n","            class_mask = (sup_lab == i)\n","            if class_mask.any():\n","                protos.append(sup_emb[class_mask].mean(0))\n","            else:\n","                protos.append(torch.zeros(self.embedding_output_dim, device=sup_emb.device))\n","        protos_st = torch.stack(protos)\n","        \n","        n_qry = qry_emb.size(0)\n","        # Prepare for relation score calculation\n","        protos_exp = protos_st.unsqueeze(0).expand(n_qry, n_w, self.embedding_output_dim)\n","        qry_exp = qry_emb.unsqueeze(1).expand(n_qry, n_w, self.embedding_output_dim)\n","        \n","        # Concatenate pairs and get relation scores\n","        concat_pairs = torch.cat((protos_exp, qry_exp), dim=2)\n","        rel_logits = self.relation_module(concat_pairs.view(-1, 2 * self.embedding_output_dim)).view(n_qry, n_w)\n","        return rel_logits\n","\n","# --- MODEL FACTORY ---\n","\n","def create_fsl_model(model_name, config):\n","    if model_name.lower() == 'protonet':\n","        return ProtoNet(config).to(config.device)\n","    elif model_name.lower() == 'relation':\n","        return RelationNet(config).to(config.device) # UPDATED\n","    else:\n","        raise ValueError(f\"Unknown model name: {model_name}\")\n","\n","# --- EPISODIC TRAINING & EVALUATION ---\n","\n","def create_balanced_episode(dataset, indices_by_class, n_way, n_shot, n_query, \n","                            oversampler=None, balance_support=True):\n","    episode_classes = random.sample(list(indices_by_class.keys()), n_way)\n","    support_samples, query_samples = [], []\n","\n","    for class_idx in episode_classes:\n","        class_indices = indices_by_class.get(class_idx, [])\n","        if len(class_indices) < n_shot + n_query:\n","             # Fallback if not enough samples, just use what's available\n","             random.shuffle(class_indices)\n","             support_indices = class_indices[:n_shot]\n","             query_indices = class_indices[n_shot:n_shot+n_query]\n","        else:\n","            selected_indices = random.sample(list(class_indices), n_shot + n_query)\n","            support_indices = selected_indices[:n_shot]\n","            query_indices = selected_indices[n_shot:]\n","        \n","        for idx in support_indices:\n","            support_samples.append(dataset[idx] + (class_idx,))\n","        for idx in query_indices:\n","            query_samples.append(dataset[idx] + (class_idx,))\n","    \n","    if not support_samples or not query_samples:\n","        return None, None, None, None\n","\n","    s_imgs, s_labs, _ = zip(*support_samples)\n","    q_imgs, q_labs, _ = zip(*query_samples)\n","    \n","    support_images, support_labels = torch.stack(s_imgs), torch.tensor(s_labs)\n","    query_images, query_labels = torch.stack(q_imgs), torch.tensor(q_labs)\n","    \n","    if balance_support and oversampler:\n","        support_images, support_labels = oversampler.balance_support_set(\n","            support_images, support_labels, target_shots_per_class=n_shot\n","        )\n","        \n","    return support_images, support_labels, query_images, query_labels\n","\n","def train_epoch_with_dynamic_balancing(model, train_dataset, train_indices_by_class, \n","                                       optimizer, config, epoch, oversampler=None):\n","    model.train()\n","    total_loss = 0\n","    oversampler = oversampler or DynamicTaskOversampler(augmentation_strength='moderate')\n","    progress_bar = tqdm(range(config.train_episodes), desc=f\"Epoch {epoch+1}\")\n","\n","    for _ in progress_bar:\n","        n_shot = config.shots[0]\n","        support_images, support_labels, query_images, query_labels = create_balanced_episode(\n","            train_dataset, train_indices_by_class, config.n_way, n_shot, config.n_query,\n","            oversampler=oversampler, balance_support=True\n","        )\n","        \n","        if support_images is None: continue\n","\n","        support_images, support_labels = support_images.to(config.device), support_labels.to(config.device)\n","        query_images, query_labels = query_images.to(config.device), query_labels.to(config.device)\n","\n","        optimizer.zero_grad()\n","\n","        # --- Model-Specific Forward Pass ---\n","        if isinstance(model, RelationNet):\n","            logits = model(support_images, support_labels, query_images, config.n_way, n_shot)\n","            loss = F.cross_entropy(logits, query_labels) # RelationNet outputs logits\n","        else: # ProtoNet\n","            support_features = model(support_images)\n","            query_features = model(query_images)\n","            prototypes = model.calculate_prototypes(support_features, support_labels, config.n_way)\n","            distances = model.calculate_distances(query_features, prototypes)\n","            log_p = F.log_softmax(-distances, dim=1)\n","            loss = F.nll_loss(log_p, query_labels)\n","        \n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","        progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1))\n","        \n","    return total_loss / config.train_episodes\n","\n","def evaluate_with_dynamic_balancing(model, dataset, indices_by_class, config, n_episodes, n_shot,\n","                                    oversampler=None, balance_support=True):\n","    model.eval()\n","    total_acc, total_loss = 0, 0\n","    all_labels, all_preds, all_probs, all_features = [], [], [], []\n","    oversampler = oversampler or DynamicTaskOversampler(augmentation_strength='light')\n","\n","    with torch.no_grad():\n","        for _ in range(n_episodes):\n","            support_images, support_labels, query_images, query_labels = create_balanced_episode(\n","                dataset, indices_by_class, config.n_way, n_shot, config.n_query,\n","                oversampler=oversampler, balance_support=balance_support\n","            )\n","            if support_images is None: continue\n","            \n","            support_images, support_labels = support_images.to(config.device), support_labels.to(config.device)\n","            query_images, query_labels = query_images.to(config.device), query_labels.to(config.device)\n","            \n","            # --- Model-Specific Forward Pass & Evaluation ---\n","            if isinstance(model, RelationNet):\n","                logits = model(support_images, support_labels, query_images, config.n_way, n_shot)\n","                loss = F.cross_entropy(logits, query_labels)\n","                probs = F.softmax(logits, dim=1)\n","                # Get embeddings for t-SNE separately\n","                query_features = model.get_embeddings(query_images)\n","            else: # ProtoNet\n","                support_features = model(support_images)\n","                query_features = model(query_images)\n","                prototypes = model.calculate_prototypes(support_features, support_labels, config.n_way)\n","                distances = model.calculate_distances(query_features, prototypes)\n","                loss = F.nll_loss(F.log_softmax(-distances, dim=1), query_labels)\n","                probs = F.softmax(-distances, dim=1)\n","\n","            _, preds = torch.max(probs, 1)\n","            total_acc += (preds == query_labels).float().mean().item()\n","            total_loss += loss.item()\n","            \n","            all_labels.extend(query_labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","            all_probs.extend(probs.cpu().numpy())\n","            all_features.extend(query_features.cpu().numpy())\n","\n","    # --- Calculate Metrics ---\n","    mean_acc = total_acc / n_episodes if n_episodes > 0 else 0\n","    mean_loss = total_loss / n_episodes if n_episodes > 0 else 0\n","    \n","    all_labels, all_preds, all_probs = np.array(all_labels), np.array(all_preds), np.array(all_probs)\n","    \n","    if len(np.unique(all_labels)) < 2:\n","        auroc, aucpr, f1, precision_val, recall_val = 0.0, 0.0, 0.0, 0.0, 0.0\n","    else:\n","        auroc = roc_auc_score(all_labels, all_probs[:, 1])\n","        precision, recall, _ = precision_recall_curve(all_labels, all_probs[:, 1])\n","        aucpr = auc(recall, precision)\n","        f1 = f1_score(all_labels, all_preds)\n","        precision_val = precision_score(all_labels, all_preds, zero_division=0)\n","        recall_val = recall_score(all_labels, all_preds, zero_division=0)\n","        \n","    return {\n","        'accuracy': mean_acc, 'loss': mean_loss, 'auroc': auroc, 'aucpr': aucpr,\n","        'f1': f1, 'precision': precision_val, 'recall': recall_val,\n","        'confusion_matrix': confusion_matrix(all_labels, all_preds),\n","        'features': np.array(all_features), 'labels': all_labels\n","    }\n","\n","# --- MAIN TRAINING & EVALUATION WORKFLOW ---\n","\n","def train_and_evaluate_domain(domain_name, domain_data, model, config):\n","    print(f\"\\n{'='*50}\\nTraining model for domain: {domain_name}\\n{'='*50}\")\n","    domain_results_dir = f\"results/{domain_name}\"\n","    os.makedirs(domain_results_dir, exist_ok=True)\n","    \n","    oversampler = DynamicTaskOversampler(augmentation_strength='moderate')\n","    train_dataset, val_dataset, test_dataset = domain_data['train']['dataset'], domain_data['val']['dataset'], domain_data['test']['dataset']\n","    train_indices, val_indices, test_indices = domain_data['train']['indices_by_class'], domain_data['val']['indices_by_class'], domain_data['test']['indices_by_class']\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=1e-4)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.n_epochs, eta_min=1e-6)\n","    \n","    train_losses, val_losses, val_metrics_history = [], [], []\n","    best_val_loss, best_model_epoch = float('inf'), -1\n","    \n","    for epoch in range(config.n_epochs):\n","        train_loss = train_epoch_with_dynamic_balancing(model, train_dataset, train_indices, optimizer, config, epoch, oversampler)\n","        train_losses.append(train_loss)\n","        \n","        val_metrics = evaluate_with_dynamic_balancing(model, val_dataset, val_indices, config, config.val_episodes, config.shots[0], oversampler)\n","        val_metrics_history.append(val_metrics)\n","        val_losses.append(val_metrics['loss'])\n","\n","        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}, Val AUROC: {val_metrics['auroc']:.4f}\")\n","\n","        if val_metrics['loss'] < best_val_loss:\n","            best_val_loss = val_metrics['loss']\n","            best_model_epoch = epoch + 1\n","            torch.save(model.state_dict(), f\"{domain_results_dir}/best_model.pth\")\n","            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n","        scheduler.step()\n","        \n","    model.load_state_dict(torch.load(f\"{domain_results_dir}/best_model.pth\"))\n","    test_metrics = evaluate_with_dynamic_balancing(model, test_dataset, test_indices, config, config.test_episodes, config.shots[0], oversampler)\n","    print(f\"\\nFinal Test Results for domain {domain_name}: Acc: {test_metrics['accuracy']:.4f}, AUROC: {test_metrics['auroc']:.4f}\")\n","\n","    # Results saving and plotting would go here...\n","    \n","    return {'domain': domain_name, 'metrics': test_metrics, 'training_losses': train_losses, 'validation_losses': val_losses, 'validation_metrics': val_metrics_history}\n","\n","def cross_domain_evaluation(source_domain_name, target_domain_name, source_data, target_data, model, config):\n","    print(f\"\\n{'='*50}\\nCross-domain: {source_domain_name} -> {target_domain_name}\\n{'='*50}\")\n","    source_model_path = f\"results/{source_domain_name}/best_model.pth\"\n","    if not os.path.exists(source_model_path):\n","        print(\"Source model not found, skipping.\")\n","        return None\n","\n","    model.load_state_dict(torch.load(source_model_path))\n","    \n","    target_test_dataset = target_data['test']['dataset']\n","    target_test_indices = target_data['test']['indices_by_class']\n","    \n","    target_metrics = evaluate_with_dynamic_balancing(\n","        model, target_test_dataset, target_test_indices, config, \n","        config.test_episodes, config.shots[0], balance_support=True\n","    )\n","    \n","    print(f\"Cross-domain results ({source_domain_name} -> {target_domain_name}): Acc: {target_metrics['accuracy']:.4f}, AUROC: {target_metrics['auroc']:.4f}\")\n","    return {'source_domain': source_domain_name, 'target_domain': target_domain_name, 'metrics': target_metrics}\n","\n","def main(model_name='protonet', save_models=True):\n","    print(f\"Starting experiment with {model_name.upper()}...\")\n","    fsl_data = prepare_fsl_data(config)\n","    \n","    all_domain_results, cross_domain_results = [], []\n","    \n","    # Within-domain training and evaluation\n","    for domain_name, domain_data in fsl_data.items():\n","        model = create_fsl_model(model_name, config)\n","        result = train_and_evaluate_domain(domain_name, domain_data, model, config)\n","        all_domain_results.append(result)\n","\n","    # Cross-domain evaluation\n","    domain_names = list(fsl_data.keys())\n","    for source_domain in domain_names:\n","        for target_domain in domain_names:\n","            if source_domain != target_domain:\n","                model = create_fsl_model(model_name, config) # Create fresh model instance\n","                cross_result = cross_domain_evaluation(\n","                    source_domain, target_domain,\n","                    fsl_data[source_domain], fsl_data[target_domain],\n","                    model, config\n","                )\n","                if cross_result:\n","                    cross_domain_results.append(cross_result)\n","                    \n","    # Final summary (visualizations are omitted for brevity but can be added back)\n","    print(\"\\n\" + \"=\"*50 + f\"\\nEXPERIMENT SUMMARY ({model_name.upper()})\\n\" + \"=\"*50)\n","    print(\"\\nWithin-domain performance:\")\n","    for result in all_domain_results:\n","        print(f\"  {result['domain']}: Accuracy: {result['metrics']['accuracy']:.4f}, AUROC: {result['metrics']['auroc']:.4f}\")\n","\n","    print(\"\\nCross-domain performance:\")\n","    for result in cross_domain_results:\n","        print(f\"  {result['source_domain']} -> {result['target_domain']}: Accuracy: {result['metrics']['accuracy']:.4f}, AUROC: {result['metrics']['auroc']:.4f}\")\n","\n","    print(\"\\nExperiment completed successfully!\")\n","    return all_domain_results, cross_domain_results\n","\n","if __name__ == \"__main__\":\n","    # You can now choose which model to run by changing the model_name\n","    # Options: 'protonet', 'relation'\n","    all_results, cross_results = main(\n","        model_name='relation',  # <-- CHANGE THIS TO 'relation' to run RelationNet\n","        save_models=True\n","    )"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":54339,"sourceId":104884,"sourceType":"datasetVersion"}],"dockerImageVersionId":31041,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"papermill":{"default_parameters":{},"duration":620.790428,"end_time":"2025-06-17T14:13:30.94839","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-17T14:03:10.157962","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}